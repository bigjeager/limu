{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手工实现线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_size = 500\n",
    "iter_n = 50\n",
    "sample_rate = 0.1\n",
    "lr = 0.001\n",
    "\n",
    "x = torch.rand([date_size, 1]) * 10\n",
    "y_true = 3 * x + 0.8 + torch.randn([date_size, 1])\n",
    "\n",
    "w = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "loss_v = []\n",
    "for i in range(iter_n):\n",
    "  # random mask \n",
    "  mask = torch.rand(date_size) < sample_rate\n",
    "  \n",
    "  y_pre = w * x[mask] + b\n",
    "  loss = (y_pre - y_true[mask]).pow(2).mean()\n",
    "  loss.backward()\n",
    "\n",
    "  # type 1: update on tensor.data object\n",
    "  # w.data = w.data - lr * w.grad\n",
    "  # b.data = b.data - lr * b.grad\n",
    "  # w.grad.data.zero_()\n",
    "  # b.grad.data.zero_()\n",
    "\n",
    "  # type 2: update on tensor object in no_grad\n",
    "  with torch.no_grad():\n",
    "    # w.sub_(lr * w.grad): work\n",
    "    # w = w - lr * w.grad: not work!!!! w's requires_grad will be False after\n",
    "    w -= lr * w.grad\n",
    "    b -= lr * b.grad\n",
    "  w.grad.zero_()\n",
    "  b.grad.zero_()\n",
    "\n",
    "  loss_v.append(loss.item())\n",
    "\n",
    "print(w.item(), b.item(), loss.item())\n",
    "\n",
    "y_pre = w * x + b\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x.numpy(), y_pre.detach().numpy(), color = 'r')\n",
    "plt.scatter(x.numpy(), y_true.numpy(), s=2)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(iter_n), loss_v)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用nn.Module实现线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "date_size = 1000\n",
    "iter_n = 3000\n",
    "sample_rate = 0.3\n",
    "lr = 0.01\n",
    "\n",
    "x = torch.rand([date_size, 1]) * 10\n",
    "y_true = torch.sin(x) + 0.8 + torch.randn([date_size, 1]) * 0.1\n",
    "\n",
    "class LR1(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(1,10)\n",
    "    self.linear2 = nn.Linear(10,1)\n",
    "    self.sig = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.linear2(self.sig(self.linear1(x)))\n",
    "    return out\n",
    "\n",
    "class LR2(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.linear = nn.Linear(1,1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.linear(x)\n",
    "    return out\n",
    "\n",
    "model = LR1()\n",
    "criteria = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "loss_v = []\n",
    "\n",
    "for i in range(iter_n):\n",
    "  # random mask \n",
    "  mask = torch.rand(date_size) < sample_rate\n",
    "  \n",
    "  y_pre = model(x[mask])\n",
    "  loss = criteria(y_pre, y_true[mask])\n",
    "  loss.backward()\n",
    "  loss_v.append(loss.item())\n",
    "\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "#    print(name, param.data)\n",
    "print(loss.item())\n",
    "\n",
    "model.eval()\n",
    "y_pre = model(x)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x.numpy(), y_pre.detach().numpy(), color = 'r', s=2)\n",
    "plt.scatter(x.numpy(), y_true.numpy(), s=2)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(iter_n), loss_v)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4504, -1.8309,  0.5252,  0.8609],\n",
      "        [ 0.2168, -0.9063,  0.0332,  0.8122],\n",
      "        [-1.3639, -0.4885,  0.0419, -0.1577]])\n",
      "140635443850272\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3,4)\n",
    "y = torch.zeros_like(x)\n",
    "\n",
    "# in_place operation\n",
    "print(id(y))\n",
    "y[:] = y + x\n",
    "print(id(y))\n",
    "y += x\n",
    "print(id(y))\n",
    "\n",
    "# no a in_place operation\n",
    "y = y + x\n",
    "print(id(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0       3.0           1          0\n",
      "1       2.0           0          1\n",
      "2       4.0           0          1\n",
      "3       3.0           0          1\n",
      "0    127500\n",
      "1    106000\n",
      "2    178100\n",
      "3    140000\n",
      "Name: Price, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 1., 0.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 0., 1.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500, 106000, 178100, 140000]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "## create data\n",
    "os.makedirs(os.path.join('data'), exist_ok=True)\n",
    "data_file = os.path.join('data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')\n",
    "    f.write('NA,Pave,127500\\n')\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')\n",
    "## load data\n",
    "data = pd.read_csv(data_file)\n",
    "input, output = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "input = input.fillna(input.mean())\n",
    "input = pd.get_dummies(input, dummy_na=True)\n",
    "print(input)\n",
    "print(output)\n",
    "## fill in torch\n",
    "X, y = torch.tensor(input.values), torch.tensor(output.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]]) tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]])\n",
      "tensor([0, 1, 4, 9])\n",
      "tensor(14) tensor(14)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "## scalar multiply with a matrix, they both output the same value\n",
    "## torch.tensor(2.0) is a scalar\n",
    "## torch.tensor([2.0]) is a one-dim tensor with one value\n",
    "print( torch.tensor(2.0) * torch.ones(5,5), torch.tensor([2.0]) * torch.ones(5,5) )\n",
    "\n",
    "## hadamard product: element-wise product\n",
    "print( torch.arange(4) * torch.arange(4))\n",
    "\n",
    "## dot product of two vector: torch.dot or torch.matmul\n",
    "print( torch.dot( torch.arange(4), torch.arange(4) ) , torch.matmul( torch.arange(4), torch.arange(4) ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0/1/2维tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero d:  tensor(4) 0 torch.Size([])\n",
      "one d:  tensor([0, 1, 2, 3]) 1 torch.Size([4])\n",
      "two d:  tensor([[0, 1, 2, 3]]) 2 torch.Size([1, 4])\n",
      "get count on specific dims  4 4\n"
     ]
    }
   ],
   "source": [
    "# tensor.dim() = element in tensor.size()\n",
    "\n",
    "# difference between shape [1,4] and [4]\n",
    "# 2d tensor[1,4] and 1d tensor[4]\n",
    "# torch.arange(4).view(1,4): [[1,2,3,4]], dim() = 2, shape = [1,4]\n",
    "# torch.arange(4):           [1,2,3,4],   dim() = 1, shape = [4]\n",
    "\n",
    "# size() and shape\n",
    "# 1. method vs property\n",
    "# 2. tensor.size(dim=x) == tensor.shape[x]\n",
    "# 3. all return torch.Size([])\n",
    "\n",
    "zero_d = torch.tensor(4)\n",
    "one_d = torch.arange(4)\n",
    "two_d = torch.arange(4).view(1,-1)\n",
    "print(\"zero d: \", zero_d, zero_d.dim(), zero_d.shape)\n",
    "print(\"one d: \", one_d, one_d.dim(), one_d.size())\n",
    "print(\"two d: \", two_d, two_d.dim(), two_d.shape)\n",
    "\n",
    "print(\"get count on specific dims \", two_d.size(dim=1), two_d.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch中的乘法：dot,mv,mm,@,matmul,mut,mutiply,*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 4, 9]) tensor([0, 1, 4, 9]) tensor([0, 1, 4, 9])\n",
      "tensor(14) tensor(14) tensor(14)\n",
      "tensor([14, 38, 62]) tensor([14, 38, 62]) tensor([14, 38, 62])\n",
      "tensor([[ 20,  23,  26,  29],\n",
      "        [ 56,  68,  80,  92],\n",
      "        [ 92, 113, 134, 155],\n",
      "        [128, 158, 188, 218]]) tensor([[ 20,  23,  26,  29],\n",
      "        [ 56,  68,  80,  92],\n",
      "        [ 92, 113, 134, 155],\n",
      "        [128, 158, 188, 218]]) tensor([[ 20,  23,  26,  29],\n",
      "        [ 56,  68,  80,  92],\n",
      "        [ 92, 113, 134, 155],\n",
      "        [128, 158, 188, 218]])\n"
     ]
    }
   ],
   "source": [
    "a_0d_4 = torch.arange(4)\n",
    "b_0d_4 = torch.arange(4)\n",
    "c_1d_43 = torch.arange(12).view(4,3)\n",
    "d_1d_34 = torch.arange(12).view(3,4)\n",
    "\n",
    "################################\n",
    "# element-wise multiply is the most simplest\n",
    "################################\n",
    "# Hadamard product: 按位乘法\n",
    "print( torch.mul(a_0d_4, b_0d_4), a_0d_4 * b_0d_4, torch.multiply(a_0d_4, b_0d_4))\n",
    "\n",
    "################################\n",
    "# torch.dot, torch.mv, torch.mm only perform on 0-d/1-d/2-d tensor\n",
    "# @ and torch.matmul are equal and more generalized version of the above method, can perform on high-dim tensor, while operate on last dim and retain the shape\n",
    "# ATTENTION: both values have same dtype\n",
    "\n",
    "# torch.dot(1d, 1d)=>0d , dot([n],   [n])   => [].   , eg: [4],  [4]   => [], tensor([0,1,2,3]), tensor([0,1,2,3]) = tensor(14) \n",
    "# torch.mv (2d, 1d)=>1d , mv ([m,n], [n])   => [m].  , eg: [3,4],[4]   => [3]\n",
    "# torch.mm (2d, 2d)=>2d , mm ([m,n], [n,x]) => [m,x] , eg: [3,4],[4,3] => [3,3]\n",
    "################################\n",
    "# dot product: 点积\n",
    "print( torch.dot(a_0d_4,b_0d_4), a_0d_4 @ b_0d_4, torch.matmul(a_0d_4,b_0d_4) )\n",
    "# mat * vec: 最后一个dim上做点积\n",
    "print( torch.mv(d_1d_34,a_0d_4), d_1d_34 @ a_0d_4, torch.matmul(d_1d_34,a_0d_4) )\n",
    "# mat * mat: 矩阵乘法\n",
    "print( torch.mm(c_1d_43,d_1d_34), c_1d_43 @ d_1d_34, torch.matmul(c_1d_43,d_1d_34) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregate function: sum/mean/max/min and L1,L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor(66.) tensor(5.5000) tensor(11.) tensor(0.)\n",
      "tensor([12., 15., 18., 21.]) tensor([[12., 15., 18., 21.]])\n",
      "tensor([12., 15., 18., 21.]) tensor([[12., 15., 18., 21.]])\n",
      "tensor([12., 15., 18., 21.]) tensor([4., 5., 6., 7.]) torch.return_types.max(\n",
      "values=tensor([ 8.,  9., 10., 11.]),\n",
      "indices=tensor([2, 2, 2, 2])) torch.return_types.min(\n",
      "values=tensor([0., 1., 2., 3.]),\n",
      "indices=tensor([0, 0, 0, 0]))\n",
      "tensor([ 6., 22., 38.]) tensor([1.5000, 5.5000, 9.5000]) torch.return_types.max(\n",
      "values=tensor([ 3.,  7., 11.]),\n",
      "indices=tensor([3, 3, 3])) torch.return_types.min(\n",
      "values=tensor([0., 4., 8.]),\n",
      "indices=tensor([0, 0, 0]))\n",
      "tensor(22.4944) tensor(66.)\n",
      "tensor(22.4944) tensor(66.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.arange(12).view(3,4).float()\n",
    "print(x1)\n",
    "print( x1.sum(), x1.mean(), x1.max(), x1.min() )\n",
    "\n",
    "# keepdim\n",
    "print( x1.sum(dim=0), x1.sum(dim=0, keepdim=True) )\n",
    "print( x1.sum(axis=0), x1.sum(axis=0, keepdim=True) )\n",
    "\n",
    "# along columns: [3,4] => [4]\n",
    "# the specified dimension will be eliminated after operation\n",
    "print( x1.sum(dim=0), x1.mean(dim=0), x1.max(dim=0), x1.min(dim=0) )\n",
    "\n",
    "# along rows:    [3,4] => [3]\n",
    "print( x1.sum(dim=1), x1.mean(dim=1), x1.max(dim=1), x1.min(dim=1) )\n",
    "\n",
    "\n",
    "y1 = torch.arange(12).float()\n",
    "y2 = torch.arange(12).view(3,4).float()\n",
    "# vector L2/L1 norm\n",
    "print( y1.norm(), y1.abs().sum())\n",
    "# matrix F norm and L1 norm\n",
    "print( y2.norm(), y2.abs().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# freeze paramter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True])\n",
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4, requires_grad=True, dtype=torch.float32)\n",
    "y = x * x          # element-wise multiply\n",
    "u = y.detach()     # u requires_grad=False\n",
    "z = u * x          # u is constant in this case\n",
    "\n",
    "# freeze y = x * x\n",
    "z.sum().backward()\n",
    "print(x.grad == u)\n",
    "\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "print(x.grad == 2*x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交换维度\n",
    "permute, transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(24).reshape(2, 3, 4)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 2]), torch.Size([2, 4, 3]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.permute(2, 1, 0).shape, x.transpose(1, 2).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "13a3192d214980b11283dc780713d8eb67d0b670d59bd5525fce4a9733b3d8cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
